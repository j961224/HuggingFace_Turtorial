# How do Transformers work?

## A bit of Transformer history

![trtrtr](https://user-images.githubusercontent.com/59636424/134766745-13afce2d-7701-42ef-9ebc-8da954efcdc7.PNG)

> * GPT (2018.6)
> > 다양한 NLP task를 수행하며 fine-tuning을 위해 사용되는 첫번째 사전학습 transformer 모델로 SOTA모델로 성과를 얻음
> * BERT (2018.10)
> > 문장 요약에 더 좋은 성과를 보여준 큰 사전학습 모델이다.
> * GPT-2 (2019.2)
> > 좀 더 커진 GPT 버전!
> * DistilBERT (2019.10)
> > BERT의 성능 97%를 보유하지만 메모리적으로 40% 가벼워지고 60% 더 빨라진 BERT의 distilled version!
> * BART & T5 (2019.10)
> > 원래 Transformer 모델로서 같은 구조로 사용되는 2개의 큰 사전학습 모델
> * GPT-3 (2020.5)
> > fine-tuning없이 다양한 task를 잘 수행하는 GPT-2의 큰 버전! (zero-shot learning)
>
